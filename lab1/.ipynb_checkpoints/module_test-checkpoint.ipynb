{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is: ===============>>>>>\t (5, 4)\n",
      "\u001b[1;34;43mThe input matrix is:\u001b[0m\n",
      "[[0.5451584  0.66827379 0.74852179 0.73265044]\n",
      " [0.1330552  0.04264401 0.59893159 0.65283652]\n",
      " [0.15681937 0.48647778 0.63007423 0.08069591]\n",
      " [0.1711248  0.36235328 0.09590372 0.27647464]\n",
      " [0.92066388 0.53578335 0.60106525 0.08613708]]\n",
      "************************************************************\n",
      "***********************\u001b[0;31mBN Layer Test\u001b[0m************************\n",
      "************************************************************\n",
      "1. Using your own Linear code.....\n",
      "\n",
      "[[ 0.51903026  1.17398195  0.94420118  1.32305515]\n",
      " [-0.81953025 -1.77374808  0.28301961  1.03523651]\n",
      " [-0.74234136  0.31742818  0.42066861 -1.02797163]\n",
      " [-0.69587561 -0.26739935 -1.94033966 -0.3219699 ]\n",
      " [ 1.73871697  0.54973729  0.29245026 -1.00835012]]\n",
      "*************The grad is as follows:**************\n",
      "[[ 0.04439359  0.02869589 -0.06908592  0.04636462]\n",
      " [-0.11159999  0.05609927 -0.04991476  0.05496713]\n",
      " [-0.06020335  0.03091153  0.06382314 -0.07409706]\n",
      " [ 0.01905295 -0.08771564  0.03493231 -0.04091025]\n",
      " [ 0.1083568  -0.02799104  0.02024523  0.01367556]]\n",
      "\n",
      "2. Here is the official code.....\n",
      "\n",
      "tensor([[ 0.5190,  1.1740,  0.9442,  1.3231],\n",
      "        [-0.8195, -1.7737,  0.2830,  1.0352],\n",
      "        [-0.7423,  0.3174,  0.4207, -1.0280],\n",
      "        [-0.6959, -0.2674, -1.9403, -0.3220],\n",
      "        [ 1.7387,  0.5497,  0.2924, -1.0084]],\n",
      "       grad_fn=<NativeBatchNormBackward>)\n",
      "*************The grad is as follows:**************\n",
      "tensor([[ 0.1022,  0.8869, -0.9277, -0.0639],\n",
      "        [-0.5993,  0.8679, -0.8487,  0.1930],\n",
      "        [-0.1115,  0.7538,  1.4348, -0.4454],\n",
      "        [ 0.6919, -2.0041, -0.1846, -0.3697],\n",
      "        [-0.0834, -0.5044,  0.5262,  0.6861]])\n"
     ]
    }
   ],
   "source": [
    "# %load module_test.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nn\n",
    "\n",
    "\n",
    "class TestBase(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.types = ['maxPooling', 'Conv2D', 'BN', 'FC']\n",
    "\n",
    "        assert self.kwargs.get('type', None) is not None, \"未指定测试模块类型，请添加'type'关键字\"\n",
    "        self.module_type = self.kwargs.get('type')\n",
    "        assert self.module_type in self.types, \"指定模块无效\"\n",
    "\n",
    "        # 判断类型选择不同初始化方式\n",
    "        self.input_numpy = None\n",
    "        if self.module_type == 'maxPooling':\n",
    "            self.input_numpy = np.random.rand(2, 2, 4, 4)\n",
    "        else:\n",
    "            self.input_numpy = np.random.rand(5, 4)\n",
    "        self.input_tensor = torch.Tensor(self.input_numpy)\n",
    "        self.input_tensor.requires_grad = True\n",
    "\n",
    "        # 根据不同的网络选择方式，预留不同的打印信息的方式\n",
    "        self.w = None\n",
    "        self.model_tensor = None\n",
    "        self.model_numpy = None\n",
    "\n",
    "    def forward(self):\n",
    "        self.output_tensor = self.model_tensor(self.input_tensor)    # 前向传播\n",
    "        self.output_numpy = self.model_numpy(self.input_numpy)      # 前向传播\n",
    "        if self.module_type == 'BN':                   # 如果类型为'BN'\n",
    "            self.output_tensor.backward(self.output_tensor_delta)        # 反向传播\n",
    "        else:\n",
    "            self.output_tensor_delta = self.output_tensor.sum()\n",
    "            self.output_numpy_delta = np.ones_like(self.output_numpy)\n",
    "            self.output_tensor_delta.backward()\n",
    "\n",
    "        self.printInfo()\n",
    "\n",
    "    def printInfo(self):\n",
    "        print(\"Input shape is: ===============>>>>>\\t\", self.input_numpy.shape)\n",
    "        print(\"\\033[1;34;43mThe input matrix is:\\033[0m\")\n",
    "        print(self.input_numpy)\n",
    "#         print(self.input_tensor)\n",
    "        if self.model_numpy == \"FC\":\n",
    "            print(\"The W matrix shape is: ===============>>>>>\\t\", self.w.shape)\n",
    "            print(\"The W matrix is:\")\n",
    "            print(self.w)\n",
    "        print(\"{:*^60}\".format(''))\n",
    "        print(\"{:*^71}\".format('\\033[0;31m' + self.module_type + ' Layer Test\\033[0m'))\n",
    "        print(\"{:*^60}\".format(''))\n",
    "\n",
    "        print(\"1. Using your own Linear code.....\\n\")\n",
    "        print(self.output_numpy)\n",
    "        print(\"{:*^50}\".format(\"The grad is as follows:\"))\n",
    "        print(self.model_numpy.backward(self.output_numpy_delta))\n",
    "        print()\n",
    "\n",
    "        print(\"2. Here is the official code.....\\n\")\n",
    "        print(self.output_tensor)\n",
    "        print(\"{:*^50}\".format(\"The grad is as follows:\"))\n",
    "        print(self.input_tensor.grad)\n",
    "\n",
    "\n",
    "class TestModule(TestBase):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TestModule, self).__init__(**kwargs)\n",
    "        # 偏置矩阵初始化\n",
    "        if self.module_type == \"FC\":\n",
    "            self.FCInit()\n",
    "        elif self.module_type == \"BN\":\n",
    "            self.BNInit()\n",
    "\n",
    "    def FCInit(self):\n",
    "        in_length = self.input_numpy.shape[1]\n",
    "        out_length = 4\n",
    "        self.w = np.random.normal(loc=0.0, scale=0.1, size=(out_length, in_length + 1))\n",
    "        self.w_tensor = torch.Tensor(self.w)\n",
    "        # 初始化torch层\n",
    "        self.model_tensor = torch.nn.Linear(in_features=in_length, out_features=out_length, bias=True)\n",
    "        self.model_tensor.bias.data = self.w_tensor[:, 0]\n",
    "        self.model_tensor.weight.data = self.w_tensor[:, 1:]\n",
    "        # 初始化numpy层\n",
    "        self.model_numpy = nn.Linear(in_length=in_length, out_length=out_length, w=self.w)\n",
    "\n",
    "    def BNInit(self):\n",
    "        self.output_numpy_delta = np.random.rand(self.input_numpy.shape[0], self.input_numpy.shape[1])\n",
    "        self.output_tensor_delta = torch.tensor(self.output_numpy_delta, requires_grad=True)\n",
    "        self.model_tensor = torch.nn.BatchNorm1d(num_features=self.input_numpy.shape[1], eps=1e-5, momentum=0.9, affine=True)\n",
    "        self.model_numpy = nn.BatchNorm1d(length=self.input_numpy.shape[1])\n",
    "\n",
    "    def __call__(self):\n",
    "        self.forward()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t = TestModule(type='BN')\n",
    "    t()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(torch.nn.BatchNorm1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cpu]",
   "language": "python",
   "name": "conda-env-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
