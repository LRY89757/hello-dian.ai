{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8599d2253bbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# %load modules.py\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from . import tensor\n",
    "\n",
    "\n",
    "class Module(object):\n",
    "    \"\"\"Base class for all neural network modules.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"If a module behaves different between training and testing,\n",
    "        its init method should inherit from this one.\"\"\"\n",
    "        self.training = True\n",
    "        self.forw = None # 存前向传播的结果， sigmoid函数用\n",
    "        self.mask = None # Relu函数用\n",
    "        \n",
    "\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:    # 这个函数可以使我们的类当作方法（也就是函数）来使用\n",
    "        \"\"\"Defines calling forward method at every call.\n",
    "        Should not be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Defines the forward propagation of the module performed at every call.\n",
    "        Should be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def backward(self, dy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Defines the backward propagation of the module.\n",
    "        \"\"\"\n",
    "        return dy\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Sets the mode of the module to training.\n",
    "        Should not be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        if 'training' in vars(self):\n",
    "            self.training = True\n",
    "        for attr in vars(self).values():\n",
    "            if isinstance(attr, Module):\n",
    "                Module.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets the mode of the module to eval.\n",
    "        Should not be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        if 'training' in vars(self):\n",
    "            self.training = False\n",
    "        for attr in vars(self).values():\n",
    "            if isinstance(attr, Module):\n",
    "                Module.eval()\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_length: int, out_length: int):\n",
    "        \"\"\"Module which applies linear transformation to input.\n",
    "\n",
    "        Args:\n",
    "            in_length: L_in from expected input shape (N, L_in).\n",
    "            out_length: L_out from output shape (N, L_out).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Initialize the weight\n",
    "        # of linear module.\n",
    "        print('hello world')\n",
    "        self.x = None\n",
    "        self.W = tensor.random((in_length, out_length))   # 初始化权重，注意权重还有一个参数为grad\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of linear module.\n",
    "\n",
    "        Args:\n",
    "            x: input of shape (N, L_in).\n",
    "        Returns:\n",
    "            out: output of shape (N, L_out).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of linear module.\n",
    "        self.x = x\n",
    "        return np.dot(x, self.W)\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward propagation of linear module.\n",
    "\n",
    "        Args:\n",
    "            dy: output delta of shape (N, L_out).\n",
    "        Returns:\n",
    "            dx: input delta of shape (N, L_in).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of linear module.\n",
    "        self.W.grad = np.dot(self.x.T, dy)\n",
    "        return self.W.grad\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class BatchNorm1d(Module):\n",
    "\n",
    "    def __init__(self, length: int, momentum: float=0.9):\n",
    "        \"\"\"Module which applies batch normalization to input.\n",
    "\n",
    "        Args:\n",
    "            length: L from expected input shape (N, L).\n",
    "            momentum: default 0.9.\n",
    "        \"\"\"\n",
    "        super(BatchNorm1d, self).__init__()\n",
    "        \n",
    "        # TODO Initialize the attributes\n",
    "        # of 1d batchnorm module.\n",
    "        L = length\n",
    "        self.gamma = tensor.ones(L) # initialize the parameters\n",
    "        self.beta = tensor.zeros(L)  # same as up\n",
    "        self.momentum = momentum\n",
    "        self.eps = 1e-5\n",
    "        self.bn_param = {}\n",
    "        self.bn_param['running_mean'] = tensor.zeros(L)\n",
    "        self.bn_param['running_var'] = tensor.ones(L)\n",
    "        self.x_hat = None\n",
    "        self.var = None\n",
    "        self.avg = None\n",
    "        self.vareps = None\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of batch norm module.\n",
    "\n",
    "        Args:\n",
    "            x: input of shape (N, L).\n",
    "        Returns:\n",
    "            out: output of shape (N, L).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of 1d batchnorm module.\n",
    "        # https://blog.csdn.net/weixin_39228381/article/details/107896863\n",
    "        # https://zhuanlan.zhihu.com/p/196277511\n",
    "        \n",
    "        running_mean = self.bn_param['running_mean']\n",
    "        running_var = self.bn_param['running_var']\n",
    "        \n",
    "        N, L = x.shape  # get the batch and the length of a sample\n",
    "        self.avg = np.sum(x, axis=0) / N # get the every sample's average\n",
    "        self.var = np.sum((x - np.tile(self.avg, (N, 1))) ** 2, axis=0) / N  # get the every sample's variance\n",
    "        self.xmu = x - self.avg\n",
    "        self.vareps = (self.var + self.eps) ** 0.5   # get the Denominators\n",
    "        self.x_hat = (x - np.tile(self.avg, (N, 1))) / np.tile(self.vareps, (N, 1))  # get the normalized sequence \n",
    "        \n",
    "        out = self.gamma * self.x_hat + self.beta\n",
    "        \n",
    "        running_mean = self.momentum * running_mean + (1 - self.momentum) * self.avg\n",
    "        running_var = self.momentum * running_var + (1 - self.momentum) * self.var\n",
    "        self.bn_param['running_mean'] = running_mean\n",
    "        self.bn_param['running_var'] = running_var\n",
    "        \n",
    "        return out\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "#     mu = self.avg\n",
    "#     xmu = x - mu\n",
    "#     sq = xmu ** 2 \n",
    "#     var = 1. / N * np.sum(sq, axis=0)\n",
    "#     sqrtvar = np.sqrt(var + eps)\n",
    "#     ivar = 1 / sqrtvar\n",
    "#     x_hat = xmu * ivar\n",
    "#     gammax = gamma * x_hat\n",
    "        \n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward propagation of batch norm module.\n",
    "\n",
    "        Args:\n",
    "            dy: output delta of shape (N, L).\n",
    "        Returns:\n",
    "            dx: input delta of shape (N, L).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of 1d batchnorm module.\n",
    "#         N, L = dy.shape\n",
    "#         var_plus_eps = self.vareps\n",
    "#         self.gamma.grad = np.sum(self.x_hat * dy, axis=0)\n",
    "#         self.beta.grad = np.sum(dy, axis=0)\n",
    "        \n",
    "#         dx_hat = dy * self.gamma   # x_hat's grad\n",
    "#         x_hat = self.x_hat\n",
    "\n",
    "# #         dx = N * dx_hat - np.sum(dx_hat, axis=0) + (1.0/N) * np.sum(dx_hat, axis=0) * np.sum(dx_hat * x_hat, axis=0) - x_hat * np.sum(dx_hat * x_hat, axis=0) \n",
    "# #         dx *= (1 - 1.0/N) / var_plus_eps\n",
    "        \n",
    "#         dx = dx_hat * (1 - 1. / N) * (1. / var_plus_eps) * (1 - 1. / (N * self.var) * self.xmu ** 2)\n",
    "#         return dx\n",
    "\n",
    "        xhat,gamma,xmu,ivar,sqrtvar,var,eps = self.x_hat, self.gamma, self.xmu, self.vareps, 1 / self.vareps, self.var, self.eps\n",
    "\n",
    "        #get the dimensions of the input/output\n",
    "        N,D = dout.shape\n",
    "\n",
    "        #step9\n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "        dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "        #step8\n",
    "        dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "        dxhat = dgammax * gamma\n",
    "\n",
    "        #step7\n",
    "        divar = np.sum(dxhat*xmu, axis=0)\n",
    "        dxmu1 = dxhat * ivar\n",
    "\n",
    "        #step6\n",
    "        dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "        #step5\n",
    "        dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "        #step4\n",
    "        dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "\n",
    "        #step3\n",
    "        dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "        #step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "\n",
    "\n",
    "        #step1\n",
    "        dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "        \n",
    "        #step0\n",
    "        dx = dx1 + dx2\n",
    "\n",
    "        return dx\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class Conv2d(Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, channels: int, kernel_size: int=3,\n",
    "                 stride: int=1, padding: int=0, bias: bool=False):\n",
    "        \"\"\"Module which applies 2D convolution to input.\n",
    "\n",
    "        Args:\n",
    "            in_channels: C_in from expected input shape (B, C_in, H_in, W_in).\n",
    "            channels: C_out from output shape (B, C_out, H_out, W_out).\n",
    "            kernel_size: default 3.\n",
    "            stride: default 1.\n",
    "            padding: default 0.\n",
    "        \"\"\"\n",
    "        # TODO Initialize the attributes\n",
    "        # of 2d convolution module.\n",
    "        r'''\n",
    "        卷积层的初始化\n",
    "\n",
    "        Parameter:\n",
    "        - W: numpy.array, (C_out, C_in, K_h, K_w)\n",
    "        - b: numpy.array, (C_out)\n",
    "        - stride: int\n",
    "        - pad: int\n",
    "        '''\n",
    "        self.W = tensor(np.random.randn(channels, in_channels, kernel_size, kernel_size))\n",
    "        # self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        # self.dW = None   self.W.grad\n",
    "        # self.db = None\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of convolution module.\n",
    "\n",
    "        Args:\n",
    "            x: input of shape (B, C_in, H_in, W_in).\n",
    "        Returns:\n",
    "            out: output of shape (B, C_out, H_out, W_out).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of 2d convolution module.\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = Conv2d_im2col(x)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W)\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward propagation of convolution module.\n",
    "        Args:\n",
    "            dy: output delta of shape (B, C_out, H_out, W_out).\n",
    "        Returns:\n",
    "            dx: input delta of shape (B, C_in, H_in, W_in).\n",
    "        \"\"\"\n",
    "        # TODO Implement backward propogation\n",
    "        # of 2d convolution module.\n",
    "        \n",
    "        def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "            N, C, H, W = input_shape\n",
    "            out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "            out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "            col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "            img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n",
    "            for y in range(filter_h):\n",
    "                y_max = y + stride * out_h\n",
    "                for x in range(filter_w):\n",
    "                    x_max = x + stride * out_w\n",
    "                    img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "            return img[:, :, pad:H + pad, pad:W + pad]\n",
    "        \n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dy\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        # self.b.grad = np.sum(dout, axis=0)\n",
    "        self.W.grad = np.dot(self.col.T, dout)\n",
    "        self.W.grad = self.W.grad.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        return dx\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class Conv2d_im2col(Conv2d):\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO Implement forward propogation of\n",
    "        # 2d convolution module using im2col method.\n",
    "        input_data = x\n",
    "        filter_h, filter_w = self.kernel_size, self.kernel_size\n",
    "        stride = self.stride\n",
    "        pad = self.pad\n",
    "        N, C, H, W = input_data.shape\n",
    "        out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "        out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "        img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride * out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "        return col\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class AvgPool(Module):\n",
    "\n",
    "    def __init__(self, kernel_size: int=2,\n",
    "                 stride: int=2, padding: int=0):\n",
    "        \"\"\"Module which applies average pooling to input.\n",
    "\n",
    "        Args:\n",
    "            kernel_size: default 2.\n",
    "            stride: default 2.\n",
    "            padding: default 0.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Initialize the attributes\n",
    "        # of average pooling module.\n",
    "        \n",
    "        \n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of average pooling module.\n",
    "\n",
    "        Args:\n",
    "            x: input of shape (B, C, H_in, W_in).\n",
    "        Returns:\n",
    "            out: output of shape (B, C, H_out, W_out).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of average pooling module.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward propagation of average pooling module.\n",
    "\n",
    "        Args:\n",
    "            dy: output delta of shape (B, C, H_out, W_out).\n",
    "        Returns:\n",
    "            dx: input delta of shape (B, C, H_in, W_in).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of average pooling module.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class MaxPool(Module):\n",
    "\n",
    "    def __init__(self, kernel_size: int=2,\n",
    "                 stride: int=2, padding: int=0):\n",
    "        \"\"\"Module which applies max pooling to input.\n",
    "\n",
    "        Args:\n",
    "            kernel_size: default 2.\n",
    "            stride: default 2.\n",
    "            padding: default 0.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Initialize the attributes\n",
    "        # of maximum pooling module.\n",
    "        self.pool_h = kernel_size\n",
    "        self.pool_w = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pad = padding\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of max pooling module.\n",
    "\n",
    "        Args:\n",
    "            x: input of shape (B, C, H_in, W_in).\n",
    "        Returns:\n",
    "            out: output of shape (B, C, H_out, W_out).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of maximum pooling module.\n",
    "        \n",
    "        # 因为对类的继承这些关系不是很清楚，不知道可不可以使用上文用到的方法来进行im2col的转化，索性在这里重新定义一个新的函数：\n",
    "        def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "            N, C, H, W = input_data.shape\n",
    "            out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "            out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "            img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "            col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "            for y in range(filter_h):\n",
    "                y_max = y + stride * out_h\n",
    "                for x in range(filter_w):\n",
    "                    x_max = x + stride * out_w\n",
    "                    col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "            col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "            return col\n",
    "\n",
    "        \n",
    "        N, C, H, W = x.shape\n",
    "        FN, C, FH, FW = 1, C, self.pool_h, self.pool_w\n",
    "        out_h = (H + 2 * self.pad - FH) // self.stride + 1\n",
    "        out_w = (W + 2 * self.pad - FW) // self.stride + 1\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col = col.reshape((N*out_h*out_w*C, -1))\n",
    "        col = np.max(col, axis=-1)\n",
    "        col = col.reshape(N, out_h, out_w, C)\n",
    "        col = col.transpose(0, 3, 1, 2)\n",
    "        return col\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward propagation of max pooling module.\n",
    "\n",
    "        Args:\n",
    "            dy: output delta of shape (B, C, H_out, W_out).\n",
    "        Returns:\n",
    "            out: input delta of shape (B, C, H_in, W_in).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of maximum pooling module.\n",
    "        def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "            N, C, H, W = input_shape\n",
    "            out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "            out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "            col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "            img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n",
    "            for y in range(filter_h):\n",
    "                y_max = y + stride * out_h\n",
    "                for x in range(filter_w):\n",
    "                    x_max = x + stride * out_w\n",
    "                    img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "            return img[:, :, pad:H + pad, pad:W + pad]\n",
    "        \n",
    "        \n",
    "        dout = dy\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        return dx\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class Dropout(Module):\n",
    "\n",
    "    def __init__(self, p: float=0.5):\n",
    "\n",
    "        # TODO Initialize the attributes\n",
    "        # of dropout module.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of dropout module.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backard(self, dy):\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of dropout module.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import pdb; pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cpu]",
   "language": "python",
   "name": "conda-env-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
