{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load functional.py\n",
    "import numpy as np\n",
    "from .modules import Module\n",
    "\n",
    "\n",
    "class Sigmoid(Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of sigmoid function.\n",
    "        self.forw = 1 / (1 + np.exp(-x))\n",
    "        return self.forw\n",
    "        \n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self, dy):\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of sigmoid function.\n",
    "        dy = dy * self.forw * (np.ones(self.forw.shape) - self.forw)\n",
    "        return dy\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of tanh function.\n",
    "        \n",
    "        \n",
    "        \n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self, dy):\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of tanh function.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of ReLU function.\n",
    "        self.mask = (x <= 0)   # 得到关于y大于小于0的真值的矩阵\n",
    "        z = y.copy()       # 深度拷贝一个y矩阵\n",
    "        z[self.mask] = 0   # 将小于零的值赋为0\n",
    "        return z   # 返回矩阵\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self, dy):\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of ReLU function.\n",
    "        dy[self.mask] = 0\n",
    "        dz = dy\n",
    "        return dz\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class Softmax(Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO Implement forward propogation\n",
    "        # of ReLU function.\n",
    "        \n",
    "        y = x - np.max(x, axis=-1, keepdims=True)\n",
    "        return np.exp(y) / np.sum(np.exp(y), axis=1, keepdims=True)   # 返回softmax处理后矩阵，利于进一步计算损失函数\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self, dy):\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of ReLU function.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class Loss(object):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "        >>> criterion = Loss(n_classes)\n",
    "        >>> ...\n",
    "        >>> for epoch in n_epochs:\n",
    "        ...     ...\n",
    "        ...     probs = model(x)\n",
    "        ...     loss = criterion(probs, target)\n",
    "        ...     model.backward(loss.backward())\n",
    "        ...     ...\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __call__(self, probs, targets):\n",
    "        self.probs = probs\n",
    "        self.targets = targets\n",
    "        self.loss = None\n",
    "        ...\n",
    "        return self\n",
    "\n",
    "    def backward(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "class SoftmaxLoss(Loss):\n",
    "\n",
    "    def __call__(self, probs, targets):\n",
    "\n",
    "        # TODO Calculate softmax loss.\n",
    "        z = Softmax(probs)    # 使用激活函数将输出矩阵归一化\n",
    "        batch_size = z.shape[0]   # 得到batch_size\n",
    "        loss = -np.sum(np.log(z[np.arange(batch_size), label])) / batch_size    # 求出平均损失误差值，使用交叉熵，利用one-hot特性得到每组输入的log值\n",
    "#                 loss = -np.sum(np.log(z[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "        self.loss = loss\n",
    "        self.probs = z\n",
    "        self.targets = targets\n",
    "        \n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of softmax loss function.\n",
    "        batch_size = self.probs.shape[0]  # 得到batch_size\n",
    "        dz = np.copy(self.probs)       # 深拷贝\n",
    "        for label_, z_ in zip(self.probs, dz):   # 由求梯度+onehot编码推出仅需在真实值所在位置减1即得梯度\n",
    "            z_[label_] -= 1\n",
    "        dz /= batch_size   # 取平均\n",
    "        return dz   # 返回梯度\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "\n",
    "    def __call__(self, probs, targets):\n",
    "\n",
    "        # TODO Calculate cross-entropy loss.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # TODO Implement backward propogation\n",
    "        # of cross-entropy loss function.\n",
    "\n",
    "        ...\n",
    "\n",
    "        # End of todo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cpu]",
   "language": "python",
   "name": "conda-env-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
